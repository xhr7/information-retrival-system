{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"cNBvMU7mrjo7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685027994083,"user_tz":-180,"elapsed":68037,"user":{"displayName":"Rahaf Masmali","userId":"05368389626403928122"}},"outputId":"1cb223bc-7224-4ee5-ba39-35253555629a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#connect with drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["#Download and import important libraries\n","!pip install -q wordcloud\n","import wordcloud\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('word_tokenize')\n","nltk.download('averaged_perceptron_tagger') \n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import io\n","import unicodedata\n","import numpy as np\n","import re\n","import os\n","from google.colab import files\n","from nltk.stem import WordNetLemmatizer \n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer \n","from sklearn.feature_extraction.text import TfidfVectorizer \n","from sklearn.metrics.pairwise import linear_kernel\n","from nltk.stem import PorterStemmer\n"],"metadata":{"id":"oUzNrasXuFEW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685028013467,"user_tz":-180,"elapsed":8916,"user":{"displayName":"Rahaf Masmali","userId":"05368389626403928122"}},"outputId":"a22e3a8a-8726-4579-fcf5-391ac0d8bfee"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Error loading word_tokenize: Package 'word_tokenize' not\n","[nltk_data]     found in index\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]}]},{"cell_type":"code","source":["\n","#Tokenization\n","def tokenize(txt):\n","  tokenizer = RegexpTokenizer(r'\\w+')\n","  tokens=tokenizer.tokenize(txt)\n","  return tokens\n","\n","\n","#stop word remove and lower case\n","stop_words = stopwords.words('english')\n","def stopwordRemove(text):\n","  words = [word for word in text if word.lower() not in stop_words]\n","  new_text=words\n","  return new_text\n","\n","\n","#stemming\n","def stemming(tokenized_text):\n","  stemmer = PorterStemmer()\n","  stemmed_words = [stemmer.stem(word) for word in tokenized_text]\n","  return stemmed_words \n","\n","#lemmatization\n","import nltk\n","lemmatizer = WordNetLemmatizer()\n","def lemmatization(tokenized_text):\n","  text =[lemmatizer.lemmatize(word) for word in tokenized_text]\n","  return text \n","\n","\n","#Create an list of files before the indexing\n","\n","def read_text_files(directory):\n","    text_files_contents = []\n","    for file in os.listdir(directory):\n","        if file.endswith('.txt'):\n","            with open(os.path.join(directory, file), 'r') as f:\n","                contents = f.read()\n","                text_files_contents.append(contents)\n","    return text_files_contents \n","\n","\n","\n","#print all elemnt in the list\n","orgenal_list=read_text_files('/content/drive/MyDrive/Documents')   \n","print(\"orignal texts : \\n\")\n","for i, string in enumerate(orgenal_list):\n","  print(i+1,\"-\",string,\"\\n\")\n","\n","\n","\n","#Create an list of files after the indexing\n","def File_after_indexing(string_list):\n","    processed_strings_list = []\n","    for string in string_list:\n","        # Perform some processing on the string\n","        removeNumbers = ''.join([i for i in string if not i.isdigit()])\n","        txtTok=tokenize(removeNumbers.lower())\n","        txtRemove=stopwordRemove(txtTok)\n","        stem=stemming(txtRemove)\n","        lem=lemmatization(stem)\n","        processed_string = ' '.join(map(str, lem ))\n","        processed_strings_list.append(processed_string)\n","    return processed_strings_list\n","\n","\n","\n","#print the list after indexing\n","new_list=File_after_indexing(orgenal_list)\n","\n","print(\"new text after pre pre-prociessing : \\n\")\n","for i, string in enumerate(new_list):\n","  print(i+1,\"-\",string,\"\\n\")\n","\n","print(\"tokenize on doc 9\")\n","t=tokenize(orgenal_list[9])\n","s=stopwordRemove(t)\n","print(t,\"\\n\") \n","print(\"after lower case and remove stop word on doc 9\")\n","print(s,\"\\n\")\n","m=stemming(s)\n","l=lemmatization(m)\n","print(\"after steming and lemmatization on doc 9\")\n","print(l,\"\\n\")\n","# Save the output to a text file\n","doc1Clean =new_list[0]\n","with open('/content/drive/MyDrive/project-final/Document-clear/Doc1-clean.txt', 'w') as f:\n","  f.write(doc1Clean)\n","\n","doc2Clean =new_list[1]\n","with open('/content/drive/MyDrive/project-final/Document-clear/Doc2-clean.txt', 'w') as f:\n","  f.write(doc2Clean)\n","\n","\n","doc3Clean =new_list[2]\n","with open('/content/drive/MyDrive/project-final/Document-clear/Doc3-clean.txt', 'w') as f:\n","  f.write(doc3Clean)\n","\n","\n","doc4Clean =new_list[3]\n","with open('/content/drive/MyDrive/project-final/Document-clear/Doc4-clean.txt', 'w') as f:\n","  f.write(doc4Clean)\n","\n","\n","doc5Clean =new_list[4]\n","with open('/content/drive/MyDrive/project-final/Document-clear/Doc5-clean.txt', 'w') as f:\n","  f.write(doc5Clean)\n","\n","\n","doc6Clean =new_list[5]\n","with open('/content/drive/MyDrive/project-final/Document-clear/Doc6-clean.txt', 'w') as f:\n","  f.write(doc6Clean)\n","\n","\n","doc7Clean =new_list[6]\n","with open('/content/drive/MyDrive/project-final/Document-clear/Doc7-clean.txt', 'w') as f:\n","  f.write(doc7Clean)\n","\n","\n","doc8Clean =new_list[7]\n","with open('/content/drive/MyDrive/project-final/Document-clear/Doc8-clean.txt', 'w') as f:\n","  f.write(doc8Clean)\n","\n","\n","doc9Clean =new_list[8]\n","with open('/content/drive/MyDrive/project-final/Document-clear/Doc9-clean.txt', 'w') as f:\n","  f.write(doc9Clean)\n","\n","\n","doc10Clean =new_list[9]\n","with open('/content/drive/MyDrive/project-final/Document-clear/Doc10-clean.txt', 'w') as f:\n","  f.write(doc10Clean)\n","\n","doc11Clean =new_list[10]\n","with open('/content/drive/MyDrive/project-final/Document-clear/Doc11-clean.txt', 'w') as f:\n","  f.write(doc11Clean)\n","\n","doc12Clean =new_list[11]\n","with open('/content/drive/MyDrive/project-final/Document-clear/Doc12-clean.txt', 'w') as f:\n","  f.write(doc12Clean)"],"metadata":{"id":"Y34nNRpzuR9K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685028048593,"user_tz":-180,"elapsed":21992,"user":{"displayName":"Rahaf Masmali","userId":"05368389626403928122"}},"outputId":"6754b63a-cd28-4774-d907-882a608b3cad"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["orignal texts : \n","\n","1 - A data structure is a specialized format for organizing, processing, retrieving and storing data. There are several basic and advanced types of data structures, \n","all designed to arrange data to suit a specific purpose. Data structures make it easy for users to access and work with the data they need in appropriate ways. \n","Most importantly, data structures frame the organization of information so that machines and humans can better understand it.In computer science and computer \n","programming, a data structure may be selected or designed to store data for the purpose of using it with various algorithms. In some cases, the algorithm's basic \n","operations are tightly coupled to the data structure's design. Each data structure contains information about the data values, relationships between the data\n","and -- in some cases -- functions that can be applied to the data. \n","\n","2 - The history of data structures dates back to the early days of computer programming. One of the earliest data structures was the array, which was used in FORTRAN\n","in the 1950s. The linked list was introduced in the 1960s, followed by the stack and queue. In the 1970s, the tree data structure was developed, and in the 1980s,\n","the hash table was introduced. Today, there are many different types of data structures, each with their own advantages and disadvantages. The development of data \n","structures has played a crucial role in the evolution of computer programming and software engineering. The development of data structures has been driven by the need \n","for efficient storage and retrieval of data in computer programs. As programs became more complex, the need for more sophisticated data structures grew. One of the most \n","significant advances in data structures was the introduction of object-oriented programming in the 1980s. Object-oriented programming allowed for the creation of complex\n","data structures that could be easily reused and modified. Today, data structures are used in a wide range of applications, from databases and search engines to video games\n","and mobile apps. The study of data structures is an important part of computer science education, and understanding them is essential for developing efficient software. \n","\n","3 - Data structures are important because they help organize and store data in an efficient and easily accessible way. They improve the performance of programs and \n","applications, make data analysis and retrieval easier, and allow developers and programmers to access data faster and more efficiently, which helps improve the \n","quality of software and applications and saves time and effort.Typical base data types, such as integers or floating-point values, that are available in most computer\n","programming languages are generally insufficient to capture the logical intent for data processing and use. Yet applications that ingest, manipulate and produce\n","information must understand how data should be organized to simplify processing. Data structures bring together the data elements in a logical way and facilitate\n","the effective use, persistence and sharing of data. They provide a formal model that describes the way the data elements are organized. \n","\n","4 - Data structures are used in a wide range of applications, including Databases: Data structures such as B-trees and hash tables are used to efficiently store\n","and retrieve large amounts of data , Search engines: Data structures such as inverted indexes are used to quickly search through large collections of documents,\n","Video games: Data structures such as graphs and trees are used to represent game worlds and game mechanics,Operating systems: Data structures such as linked lists\n","and queues are used to manage system resources such as memory and processes, Mobile apps: Data structures such as arrays and maps are used to store and manipulate \n","data in mobile applications, Artificial intelligence: Data structures such as decision trees and neural networks are used in machine learning and other AI applications,\n","Web development: Data structures such as stacks and queues are used to manage web server requests and responses.These are just a few examples of the many applications \n","of data structures in computer programming and software engineering. \n","\n","5 - Basically, data structures are divided into two categories Linear data structure and Non-linear data structure The main difference between linear and non-linear\n","data structures is the way in which they organize and store data.Linear data structures store data elements in a linear sequence, where each element is connected \n","to its previous and next element. Examples of linear data structures include arrays, linked lists, stacks, and queues. Linear data structures are useful when data \n","needs to be accessed sequentially, or when data elements need to be added or removed from one end of the structure.Non-linear data structures, on the other hand, \n","do not store data elements in a linear sequence. Instead, data elements are organized in a hierarchical or network-like structure. Examples of non-linear data \n","structures include trees and graphs. Non-linear data structures are useful when data needs to be organized based on relationships between data elements, or when \n","data needs to be accessed in a non-sequential manner. \n","\n","6 - An array is a  linear data structure that stores a collection of elements of the same data type. In Computer Programming,an array assembles equivalent data \n","elements stored at adjacent memory locations. It is a fixed-size container that can hold a specific number of elements of the same data type , This easiest \n","data structure helps to determine the location of each piece of data by simply using an offset to the base value. An offset is a number that illustrates the\n","distinction between two index numbers.For instance, If you want to keep a record of the marks obtained by a student in 6 subjects, then you don’t need to determine\n","the variables individually. Rather, you can determine an array that will store each piece of data factors at a contiguous memory location.Arrays are one of the oldest\n","and most essential data structures used in almost every program. Arrays are also useful in implementing other data structures such as lists, heaps, hash tables, \n","deques, queues, stacks, and strings. \n","\n","7 - stack is a type of linear data structure that stores a collection of elements  is a logical concept that consists of a set of similar elements. The term is \n","often used in programming and memory organization in computers. Programming stacks are based on the principle of last in first out (LIFO), a commonly used type\n","of data abstract that consists of two major operations, push and pop. The push operation adds an element to the bottom of stack while the pop operation removes\n","an element from the top position.Software implementations of the stack concept are done using arrays and linked lists where the top position is tracked using \n","a variable or header pointer respectively. Many programming languages provide built-in features to support stack implementation. Stack is used in many software \n","applications, including artificial intelligence applications, animation applications, medical applications, and banking and financial applications.is also used\n","in many programming languages \n","\n","8 - a queue is is a type of linear data structure that stores a collection of elements  is an ordered collection of items where the addition of new items happens\n","at one end, called the “rear,” and the removal of existing items occurs at the other end, commonly called the “front.” As an element enters the queue it starts\n","at the rear and makes its way toward the front, waiting until that time when it is the next element to be removed.The most recently added item in the queue must\n","wait at the end of the collection. The item that has been in the collection the longest is at the front. This ordering principle is sometimes called FIFO, first-in\n","first-out. It is also known as “first-come first-served\".The simplest example of a queue is the typical line that we all participate in from time to time. We wait\n","in a line for a movie, we wait in the check-out line at a grocery store, and we wait in the cafeteria line (so that we can pop the tray stack). Well-behaved lines,\n","or queues, are very restrictive in that they have only one way in and only one way out. Queues are commonly used in programming languages to manage tasks that need\n","to be performed in a specific order, such as printing jobs or processing requests. Queues can be implemented using arrays or linked lists. \n","\n","9 - tree data structure is a collection of nodes connected by edges. Each node contains a value or data which may or may not have a child node. The first node of\n","the tree is called the root. If this root node is connected with another node, then this root is called the parent node, and the node connected to it is the \n","child node. When operations are performed in a linear data structure, the complexity rises as the data size increases. On the other hand, the tree data structure\n","provides much quicker access to the data, which is non-linear. Trees are commonly used to represent hierarchical relationships, such as file systems, organization \n","charts, and HTML documents. There are many different types of trees, such as binary trees. \n","\n","10 - Graphs are non-linear data structures that are made up of a set of nodes (or vertices), connected by edges (or arcs). Nodes are entities where the data is stored\n","and their relationships are expressed using edges. Edges may be directed or undirected. Graphs demonstrate complicated relationships with ease and are used to\n","solve many real-life problems.The most common types of graphs in the data structure are Undirected graph, Directed graph, Weighted Graph, Unweighted .Graphs are \n","powerful data structures that represent real-life relationships between entities. Graphs are used everywhere, from social networks, Google maps, and the world wide\n","web to blockchains and neural networks. Due to their ability to provide abstractions to real-life, they are used in a variety of practical problems. \n","\n","11 - Birds are vertebrate animals adapted for flight. Many can also run, jump, swim, and dive. Some, like penguins, have lost the ability to fly but retained their\n","wings. Birds are found worldwide and in all habitats. The largest is the nine-foot-tall ostrich. The smallest is the two-inch-long bee hummingbird. Everything\n","about the anatomy of a bird reflects its ability to fly. The wings, for example, are shaped to create lift. The leading edge is thicker than the back edge, and \n","they are covered in feathers that narrow to a point. Airplane wings are modeled after bird wings.Birds have a unique digestive system that allows them to eat when \n","they can—usually on the fly—and digest later.  \n","\n","12 - Saffron, a spice derived from the dried stigmas of the saffron crocus , has remained among the world's most costly substances throughout history. With its bitter\n","taste, hay-like fragrance, and slight metallic notes, saffron has been used as a seasoning, fragrance, dye, and medicine. Saffron is native to Southwest Asia \n","but was first cultivated in Greece. Saffron’s exorbitant cost is due to the laborious task of harvesting. First, farmers separate the delicate strands from \n","each flower manually. Then, they heat and cure the threads to enhance their flavour. The extra labour amounts to the cost and benefits of saffron. Hence, it \n","is also known as the king of spices. \n","\n","new text after pre pre-prociessing : \n","\n","1 - data structur special format organ process retriev store data sever basic advanc type data structur design arrang data suit specif purpos data structur make easi user access work data need appropri way importantli data structur frame organ inform machin human better understand comput scienc comput program data structur may select design store data purpos use variou algorithm case algorithm basic oper tightli coupl data structur design data structur contain inform data valu relationship data case function appli data \n","\n","2 - histori data structur date back earli day comput program one earliest data structur array use fortran link list introduc follow stack queue tree data structur develop hash tabl introduc today mani differ type data structur advantag disadvantag develop data structur play crucial role evolut comput program softwar engin develop data structur driven need effici storag retriev data comput program program becam complex need sophist data structur grew one signific advanc data structur introduct object orient program object orient program allow creation complex data structur could easili reus modifi today data structur use wide rang applic databas search engin video game mobil app studi data structur import part comput scienc educ understand essenti develop effici softwar \n","\n","3 - data structur import help organ store data effici easili access way improv perform program applic make data analysi retriev easier allow develop programm access data faster effici help improv qualiti softwar applic save time effort typic base data type integ float point valu avail comput program languag gener insuffici captur logic intent data process use yet applic ingest manipul produc inform must understand data organ simplifi process data structur bring togeth data element logic way facilit effect use persist share data provid formal model describ way data element organ \n","\n","4 - data structur use wide rang applic includ databas data structur b tree hash tabl use effici store retriev larg amount data search engin data structur invert index use quickli search larg collect document video game data structur graph tree use repres game world game mechan oper system data structur link list queue use manag system resourc memori process mobil app data structur array map use store manipul data mobil applic artifici intellig data structur decis tree neural network use machin learn ai applic web develop data structur stack queue use manag web server request respons exampl mani applic data structur comput program softwar engin \n","\n","5 - basic data structur divid two categori linear data structur non linear data structur main differ linear non linear data structur way organ store data linear data structur store data element linear sequenc element connect previou next element exampl linear data structur includ array link list stack queue linear data structur use data need access sequenti data element need ad remov one end structur non linear data structur hand store data element linear sequenc instead data element organ hierarch network like structur exampl non linear data structur includ tree graph non linear data structur use data need organ base relationship data element data need access non sequenti manner \n","\n","6 - array linear data structur store collect element data type comput program array assembl equival data element store adjac memori locat fix size contain hold specif number element data type easiest data structur help determin locat piec data simpli use offset base valu offset number illustr distinct two index number instanc want keep record mark obtain student subject need determin variabl individu rather determin array store piec data factor contigu memori locat array one oldest essenti data structur use almost everi program array also use implement data structur list heap hash tabl dequ queue stack string \n","\n","7 - stack type linear data structur store collect element logic concept consist set similar element term often use program memori organ comput program stack base principl last first lifo commonli use type data abstract consist two major oper push pop push oper add element bottom stack pop oper remov element top posit softwar implement stack concept done use array link list top posit track use variabl header pointer respect mani program languag provid built featur support stack implement stack use mani softwar applic includ artifici intellig applic anim applic medic applic bank financi applic also use mani program languag \n","\n","8 - queue type linear data structur store collect element order collect item addit new item happen one end call rear remov exist item occur end commonli call front element enter queue start rear make way toward front wait time next element remov recent ad item queue must wait end collect item collect longest front order principl sometim call fifo first first also known first come first serv simplest exampl queue typic line particip time time wait line movi wait check line groceri store wait cafeteria line pop tray stack well behav line queue restrict one way one way queue commonli use program languag manag task need perform specif order print job process request queue implement use array link list \n","\n","9 - tree data structur collect node connect edg node contain valu data may may child node first node tree call root root node connect anoth node root call parent node node connect child node oper perform linear data structur complex rise data size increas hand tree data structur provid much quicker access data non linear tree commonli use repres hierarch relationship file system organ chart html document mani differ type tree binari tree \n","\n","10 - graph non linear data structur made set node vertic connect edg arc node entiti data store relationship express use edg edg may direct undirect graph demonstr complic relationship ea use solv mani real life problem common type graph data structur undirect graph direct graph weight graph unweight graph power data structur repres real life relationship entiti graph use everywher social network googl map world wide web blockchain neural network due abil provid abstract real life use varieti practic problem \n","\n","11 - bird vertebr anim adapt flight mani also run jump swim dive like penguin lost abil fli retain wing bird found worldwid habitat largest nine foot tall ostrich smallest two inch long bee hummingbird everyth anatomi bird reflect abil fli wing exampl shape creat lift lead edg thicker back edg cover feather narrow point airplan wing model bird wing bird uniqu digest system allow eat usual fli digest later \n","\n","12 - saffron spice deriv dri stigma saffron crocu remain among world costli substanc throughout histori bitter tast hay like fragranc slight metal note saffron use season fragranc dye medicin saffron nativ southwest asia first cultiv greec saffron exorbit cost due labori task harvest first farmer separ delic strand flower manual heat cure thread enhanc flavour extra labour amount cost benefit saffron henc also known king spice \n","\n","tokenize on doc 9\n","['Graphs', 'are', 'non', 'linear', 'data', 'structures', 'that', 'are', 'made', 'up', 'of', 'a', 'set', 'of', 'nodes', 'or', 'vertices', 'connected', 'by', 'edges', 'or', 'arcs', 'Nodes', 'are', 'entities', 'where', 'the', 'data', 'is', 'stored', 'and', 'their', 'relationships', 'are', 'expressed', 'using', 'edges', 'Edges', 'may', 'be', 'directed', 'or', 'undirected', 'Graphs', 'demonstrate', 'complicated', 'relationships', 'with', 'ease', 'and', 'are', 'used', 'to', 'solve', 'many', 'real', 'life', 'problems', 'The', 'most', 'common', 'types', 'of', 'graphs', 'in', 'the', 'data', 'structure', 'are', 'Undirected', 'graph', 'Directed', 'graph', 'Weighted', 'Graph', 'Unweighted', 'Graphs', 'are', 'powerful', 'data', 'structures', 'that', 'represent', 'real', 'life', 'relationships', 'between', 'entities', 'Graphs', 'are', 'used', 'everywhere', 'from', 'social', 'networks', 'Google', 'maps', 'and', 'the', 'world', 'wide', 'web', 'to', 'blockchains', 'and', 'neural', 'networks', 'Due', 'to', 'their', 'ability', 'to', 'provide', 'abstractions', 'to', 'real', 'life', 'they', 'are', 'used', 'in', 'a', 'variety', 'of', 'practical', 'problems'] \n","\n","after lower case and remove stop word on doc 9\n","['Graphs', 'non', 'linear', 'data', 'structures', 'made', 'set', 'nodes', 'vertices', 'connected', 'edges', 'arcs', 'Nodes', 'entities', 'data', 'stored', 'relationships', 'expressed', 'using', 'edges', 'Edges', 'may', 'directed', 'undirected', 'Graphs', 'demonstrate', 'complicated', 'relationships', 'ease', 'used', 'solve', 'many', 'real', 'life', 'problems', 'common', 'types', 'graphs', 'data', 'structure', 'Undirected', 'graph', 'Directed', 'graph', 'Weighted', 'Graph', 'Unweighted', 'Graphs', 'powerful', 'data', 'structures', 'represent', 'real', 'life', 'relationships', 'entities', 'Graphs', 'used', 'everywhere', 'social', 'networks', 'Google', 'maps', 'world', 'wide', 'web', 'blockchains', 'neural', 'networks', 'Due', 'ability', 'provide', 'abstractions', 'real', 'life', 'used', 'variety', 'practical', 'problems'] \n","\n","after steming and lemmatization on doc 9\n","['graph', 'non', 'linear', 'data', 'structur', 'made', 'set', 'node', 'vertic', 'connect', 'edg', 'arc', 'node', 'entiti', 'data', 'store', 'relationship', 'express', 'use', 'edg', 'edg', 'may', 'direct', 'undirect', 'graph', 'demonstr', 'complic', 'relationship', 'ea', 'use', 'solv', 'mani', 'real', 'life', 'problem', 'common', 'type', 'graph', 'data', 'structur', 'undirect', 'graph', 'direct', 'graph', 'weight', 'graph', 'unweight', 'graph', 'power', 'data', 'structur', 'repres', 'real', 'life', 'relationship', 'entiti', 'graph', 'use', 'everywher', 'social', 'network', 'googl', 'map', 'world', 'wide', 'web', 'blockchain', 'neural', 'network', 'due', 'abil', 'provid', 'abstract', 'real', 'life', 'use', 'varieti', 'practic', 'problem'] \n","\n"]}]},{"cell_type":"code","source":["\n","# number of repetitions\n","# Create the Document Term Matrix\n","count_vectorizer = CountVectorizer(stop_words='english')\n","count_vectorizer = CountVectorizer()\n","sparse_matrix = count_vectorizer.fit_transform(new_list)\n","doc_term_matrix = sparse_matrix.todense()\n","df = pd.DataFrame(doc_term_matrix,columns=count_vectorizer.get_feature_names_out())\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"8OruCjr5xsV9","executionInfo":{"status":"ok","timestamp":1685028192097,"user_tz":-180,"elapsed":650,"user":{"displayName":"Rahaf Masmali","userId":"05368389626403928122"}},"outputId":"99e98b82-0ae0-49af-8307-b5d4877116c4"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    abil  abstract  access  ad  adapt  add  addit  adjac  advanc  advantag  \\\n","0      0         0       1   0      0    0      0      0       1         0   \n","1      0         0       0   0      0    0      0      0       1         1   \n","2      0         0       2   0      0    0      0      0       0         0   \n","3      0         0       0   0      0    0      0      0       0         0   \n","4      0         0       2   1      0    0      0      0       0         0   \n","5      0         0       0   0      0    0      0      1       0         0   \n","6      0         1       0   0      0    1      0      0       0         0   \n","7      0         0       0   1      0    0      1      0       0         0   \n","8      0         0       1   0      0    0      0      0       0         0   \n","9      1         1       0   0      0    0      0      0       0         0   \n","10     2         0       0   0      1    0      0      0       0         0   \n","11     0         0       0   0      0    0      0      0       0         0   \n","\n","    ...  way  web  weight  well  wide  wing  work  world  worldwid  yet  \n","0   ...    1    0       0     0     0     0     1      0         0    0  \n","1   ...    0    0       0     0     1     0     0      0         0    0  \n","2   ...    3    0       0     0     0     0     0      0         0    1  \n","3   ...    0    2       0     0     1     0     0      1         0    0  \n","4   ...    1    0       0     0     0     0     0      0         0    0  \n","5   ...    0    0       0     0     0     0     0      0         0    0  \n","6   ...    0    0       0     0     0     0     0      0         0    0  \n","7   ...    3    0       0     1     0     0     0      0         0    0  \n","8   ...    0    0       0     0     0     0     0      0         0    0  \n","9   ...    0    1       1     0     1     0     0      1         0    0  \n","10  ...    0    0       0     0     0     4     0      0         1    0  \n","11  ...    0    0       0     0     0     0     0      1         0    0  \n","\n","[12 rows x 429 columns]"],"text/html":["\n","  <div id=\"df-8923a5c3-247c-4ede-8d9e-d9314cd97ad7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abil</th>\n","      <th>abstract</th>\n","      <th>access</th>\n","      <th>ad</th>\n","      <th>adapt</th>\n","      <th>add</th>\n","      <th>addit</th>\n","      <th>adjac</th>\n","      <th>advanc</th>\n","      <th>advantag</th>\n","      <th>...</th>\n","      <th>way</th>\n","      <th>web</th>\n","      <th>weight</th>\n","      <th>well</th>\n","      <th>wide</th>\n","      <th>wing</th>\n","      <th>work</th>\n","      <th>world</th>\n","      <th>worldwid</th>\n","      <th>yet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>12 rows × 429 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8923a5c3-247c-4ede-8d9e-d9314cd97ad7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8923a5c3-247c-4ede-8d9e-d9314cd97ad7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8923a5c3-247c-4ede-8d9e-d9314cd97ad7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["\n","# In this line we initiated words_set in which it reads each line in new_list(documents clean), which contains all the 12 filess' contents and/or strings\n","words_set = set()\n","for doc in new_list:\n","    words = doc.split(' ')\n","    words_set= words_set.union(set(words))\n","\n","#this method will preform both TF, IDF and TF * IDF using scikit-learn/sklearn (Python library)\n","vectorizer= TfidfVectorizer() # here we used TfidVectorizer to a collection of raw doucments into a matrix of TF-IDF\n","\n","#Representation\n","tfidf_matrix = vectorizer.fit_transform(new_list) #this method will fits the TfidVectorizer to the input data and transforms it into a TF-IDF representation\n","tf_idf_array = tfidf_matrix.toarray() # this method will convert the sparse matrix of TF-IDF representation into a dense matrix repesentation as a NumPy array.\n","\n","words_set =vectorizer.get_feature_names_out() # this method will return all the features names in the TF-IDF matrix. \n","\n","df_tf_idf=pd.DataFrame(tf_idf_array, columns= words_set) # this method will creates DataFrame object from a 2D NumPy array repersentring a TF-IDF matrix, with column names specfied by the words sets. \n","df_tf_idf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"id":"aEqqYNcAx7IN","executionInfo":{"status":"ok","timestamp":1683699315012,"user_tz":-180,"elapsed":807,"user":{"displayName":"Rahaf Masmali","userId":"05368389626403928122"}},"outputId":"b79ae8a1-ea96-4480-8362-4440811aae12"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        abil  abstract    access        ad     adapt       add    addit  \\\n","0   0.000000  0.000000  0.068292  0.000000  0.000000  0.000000  0.00000   \n","1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n","2   0.000000  0.000000  0.143019  0.000000  0.000000  0.000000  0.00000   \n","3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n","4   0.000000  0.000000  0.093293  0.058832  0.000000  0.000000  0.00000   \n","5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n","6   0.000000  0.082349  0.000000  0.000000  0.000000  0.095887  0.00000   \n","7   0.000000  0.000000  0.000000  0.059636  0.000000  0.000000  0.06944   \n","8   0.000000  0.000000  0.059473  0.000000  0.000000  0.000000  0.00000   \n","9   0.078398  0.078398  0.000000  0.000000  0.000000  0.000000  0.00000   \n","10  0.167979  0.000000  0.000000  0.000000  0.097797  0.000000  0.00000   \n","11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n","\n","       adjac    advanc  advantag  ...       way       web    weight     well  \\\n","0   0.000000  0.086132  0.000000  ...  0.068292  0.000000  0.000000  0.00000   \n","1   0.000000  0.072293  0.084178  ...  0.000000  0.000000  0.000000  0.00000   \n","2   0.000000  0.000000  0.000000  ...  0.214528  0.000000  0.000000  0.00000   \n","3   0.000000  0.000000  0.000000  ...  0.000000  0.160363  0.000000  0.00000   \n","4   0.000000  0.000000  0.000000  ...  0.046647  0.000000  0.000000  0.00000   \n","5   0.096133  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.00000   \n","6   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.00000   \n","7   0.000000  0.000000  0.000000  ...  0.141852  0.000000  0.000000  0.06944   \n","8   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.00000   \n","9   0.000000  0.000000  0.000000  ...  0.000000  0.078398  0.091287  0.00000   \n","10  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.00000   \n","11  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.00000   \n","\n","        wide      wing      work     world  worldwid       yet  \n","0   0.000000  0.000000  0.100292  0.000000  0.000000  0.000000  \n","1   0.063861  0.000000  0.000000  0.000000  0.000000  0.000000  \n","2   0.000000  0.000000  0.000000  0.000000  0.000000  0.105016  \n","3   0.070829  0.000000  0.000000  0.070829  0.000000  0.000000  \n","4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n","5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n","6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n","7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n","8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n","9   0.069254  0.000000  0.000000  0.069254  0.000000  0.000000  \n","10  0.000000  0.391189  0.000000  0.000000  0.097797  0.000000  \n","11  0.000000  0.000000  0.000000  0.076942  0.000000  0.000000  \n","\n","[12 rows x 429 columns]"],"text/html":["\n","  <div id=\"df-79212f0b-a0c5-4897-a025-efb7732d1e09\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abil</th>\n","      <th>abstract</th>\n","      <th>access</th>\n","      <th>ad</th>\n","      <th>adapt</th>\n","      <th>add</th>\n","      <th>addit</th>\n","      <th>adjac</th>\n","      <th>advanc</th>\n","      <th>advantag</th>\n","      <th>...</th>\n","      <th>way</th>\n","      <th>web</th>\n","      <th>weight</th>\n","      <th>well</th>\n","      <th>wide</th>\n","      <th>wing</th>\n","      <th>work</th>\n","      <th>world</th>\n","      <th>worldwid</th>\n","      <th>yet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.068292</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.086132</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.068292</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.100292</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.072293</td>\n","      <td>0.084178</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.063861</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.143019</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.214528</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.105016</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.160363</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.070829</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.070829</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.093293</td>\n","      <td>0.058832</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.046647</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.096133</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.000000</td>\n","      <td>0.082349</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.095887</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.059636</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.06944</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.141852</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.06944</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.059473</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.078398</td>\n","      <td>0.078398</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.078398</td>\n","      <td>0.091287</td>\n","      <td>0.00000</td>\n","      <td>0.069254</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.069254</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.167979</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.097797</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.391189</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.097797</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.076942</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>12 rows × 429 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79212f0b-a0c5-4897-a025-efb7732d1e09')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-79212f0b-a0c5-4897-a025-efb7732d1e09 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-79212f0b-a0c5-4897-a025-efb7732d1e09');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["\n","#enter the query from user and save in the file and indexing the query\n","query =input(\"Enter your query:\")\n","txtTok=tokenize(query.lower())\n","txtRemove=stopwordRemove(txtTok)\n","stem=stemming(txtRemove)\n","lem=lemmatization(stem)\n","stemmed_query = ' '.join(map(str, lem ))\n","print(stemmed_query)\n","\n","with open('/content/drive/MyDrive/project-final/query/query.txt', 'w') as f: f.write(query)\n"],"metadata":{"id":"g_gHjDHvyGMB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683699452867,"user_tz":-180,"elapsed":6120,"user":{"displayName":"Rahaf Masmali","userId":"05368389626403928122"}},"outputId":"d82626f4-c6d4-45d7-ed5f-c25b14090de5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter your query:tree\n","tree\n"]}]},{"cell_type":"code","source":["\n","#compute the dot proudct\n","def compute_dot_products(query):\n","    \n","    query_vector = vectorizer.transform([query])\n","    dot_products = tfidf_matrix.dot(query_vector.T).toarray().flatten()\n","\n","#sort the revelnt doucment\n","def retrieve_indices(query, text_matrix):\n","    dot_products = compute_dot_products(query)\n","    indices = np.where(dot_products > 0)[0]\n","    sorted_indices = indices[np.argsort(dot_products[indices])[::-1]]\n","    return sorted_indices\n","\n","\n","#print\n","dot_prouct= compute_dot_products(stemmed_query)\n","print(\"Dot product similarity scores:\")\n","for i in range(12):\n","  print(\"Document\",i+1,\":\",dot_prouct[i])\n","\n","print(\"\\n\")\n","\n","indices = retrieve_indices(stemmed_query,new_list)\n","if dot_prouct.max() == 0:\n","    print(\"No matching documents found.\")\n","else:\n","    print(\"Documents ranked by similarity to the query:\")\n","    for index in indices:\n","        print(index+1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VKE7df9J0MHH","executionInfo":{"status":"ok","timestamp":1683699458411,"user_tz":-180,"elapsed":453,"user":{"displayName":"Rahaf Masmali","userId":"05368389626403928122"}},"outputId":"d5e37a08-d41b-4040-e67a-5a329176734f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dot product similarity scores:\n","Document 1 : 0.0\n","Document 2 : 0.05731989398225024\n","Document 3 : 0.0\n","Document 4 : 0.19072358447267831\n","Document 5 : 0.04664656089787449\n","Document 6 : 0.0\n","Document 7 : 0.0\n","Document 8 : 0.0\n","Document 9 : 0.3568360774315552\n","Document 10 : 0.0\n","Document 11 : 0.0\n","Document 12 : 0.0\n","\n","\n","Documents ranked by similarity to the query:\n","9\n","4\n","2\n","5\n"]}]}]}